---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

permalink: /
title: Home
layout: home
---

## Has it Trained Yet?

#### A Workshop for Algorithmic Efficiency in Practical Neural Network Training

##### Friday, December 2, 2022, New Orleans at [NeurIPS 2022](https://nips.cc), **<a href="mailto:hityworkshop@gmail.com">hityworkshop@gmail.com</a>**

&nbsp;

> We all think we know how to train neural nets, but we seem to have different ideas. Let's discuss which methods truly speed up training!

&nbsp;

### News

- **August 24, 2022:**\
  &emsp; üì¢ Published the [Call for Papers](https://hity-workshop.github.io/NeurIPS2022/callforpapers/).
- **July 06, 2022:**\
  &emsp; ‚úÖ The workshop was accepted at [NeurIPS 2022](https://nips.cc/) for an in-person workshop!
- **June 2, 2022:** \
  &emsp; üñ•Ô∏è The workshop website is live!

&nbsp;

### Workshop Description

Training contemporary neural networks is a lengthy and often costly process, both in human designer time and compute resources.
Although the field has invented numerous approaches, neural network training still usually involves an inconvenient amount of "babysitting" to get the model to train properly.
This not only requires enormous compute resources but also makes deep learning less accessible to outsiders and newcomers.
This workshop will be centered around the question **"How can we train neural networks faster"** by focusing on the effects **algorithms** (not hardware or software developments) have on the training time of neural networks.
These algorithmic improvements can come in the form of novel methods, e.g. new optimizers or more efficient data selection strategies, or through empirical experience, e.g. best practices for quickly identifying well-working hyperparameter settings or informative metrics to monitor during training.

**We all think we know how to train deep neural networks, but we all seem to have different ideas.**
Ask any deep learning practitioner about the best practices of neural network training, and you will often hear a collection of arcane recipes.
Frustratingly, these hacks vary wildly between companies and teams.
This workshop offers a platform to talk about these ideas, agree on what is actually known, and what is just noise.
In this sense, this will **not** be an "optimization workshop" in the mathematical sense (of which there have been several in the past, of course).

To this end, the workshop's goal is to connect two communities:
Researchers who develop new algorithms for faster neural network training, such as new optimization methods or deep learning architectures.
Practitioners who, through their work on real-world problems, are increasingly relying on "tricks of the trade".
The workshop aims to close the gap between research and applications, identifying the most relevant current issues that hinder faster neural network training in practice.

&nbsp;

### Topics

Among the topics addressed by the workshop are:

- What "best practices" for faster neural network training are used in practice and can we learn from them to build better algorithms?
- What are painful lessons learned while training deep learning models?
- What are the most needed algorithmic improvements for neural network training?
- How can we ensure that research on training methods for deep learning has practical relevance?

&nbsp;

### Important Dates

- **Submission Deadline:**\
  &emsp; September 22, 2022, 07:00am UTC
- **Accept/Reject Notification Date:** \
  &emsp; October 14, 2022, 07:00am UTC
- **Workshop Date:**\
  &emsp; December 2, 2022

&nbsp;
